{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83be7038-5bd7-4bbd-b428-5a9b4ee45bcd",
   "metadata": {},
   "source": [
    "##  Understanding Text preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35b28d-5525-4a3e-8606-b831a3615f38",
   "metadata": {},
   "source": [
    "### Tokenization Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4375d91-4a8f-45e5-b703-c4316a5709c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/raman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/raman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/raman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/raman/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4c45a1-53e1-4260-850e-3635b1a4a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Hello welcome to alwaysoversleeping desktop.\n",
    "please do watch my body as it fragiule.\n",
    "you can clean using fibre cloth and keyboard cleaner.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c7d5c4-4d62-4beb-8d15-2d1578480a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to alwaysoversleeping desktop.\n",
      "please do watch my body as it fragiule.\n",
      "you can clean using fibre cloth and keyboard cleaner.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38059a7f-958c-4c4b-a3c6-9881d52ead8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization -> sentence\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "048bbf12-aeae-45fb-a1ff-f9e59f56ba38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello welcome to alwaysoversleeping desktop.',\n",
       " 'please do watch my body as it fragiule.',\n",
       " 'you can clean using fibre cloth and keyboard cleaner.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = sent_tokenize(corpus)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a14031f-6239-4b4b-822a-86214e5a67e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ef2e81-9b36-43fd-a0d3-1cec4784a780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello welcome to alwaysoversleeping desktop.\n",
      "please do watch my body as it fragiule.\n",
      "you can clean using fibre cloth and keyboard cleaner.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c284e9-36a3-4342-afef-45f3b3fa7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization -> word tokens\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa3bc7ac-8ae2-4fbb-bf7d-ec13abbac24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'alwaysoversleeping', 'desktop', '.', 'please', 'do', 'watch', 'my', 'body', 'as', 'it', 'fragiule', '.', 'you', 'can', 'clean', 'using', 'fibre', 'cloth', 'and', 'keyboard', 'cleaner', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = word_tokenize(corpus)\n",
    "print(doc)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70acb9a-a051-41fe-82ce-3e788f2128b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d3d16c-bb41-46e5-98cc-2538bfc0d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'alwaysoversleeping', 'desktop', '.', 'please', 'do', 'watch', 'my', 'body', 'as', 'it', 'fragiule', '.', 'you', 'can', 'clean', 'using', 'fibre', 'cloth', 'and', 'keyboard', 'cleaner', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = wordpunct_tokenize(corpus)\n",
    "print(doc1)\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3535c132-6c9a-4e53-a09f-4f69fb777e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer  # it place . , etc with end word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f61f2d9-a702-4773-b9a1-0b73129a1d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'alwaysoversleeping', 'desktop.', 'please', 'do', 'watch', 'my', 'body', 'as', 'it', 'fragiule.', 'you', 'can', 'clean', 'using', 'fibre', 'cloth', 'and', 'keyboard', 'cleaner', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer().tokenize(corpus)\n",
    "print(tokenizer)\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7635646-de96-4a33-85a4-b7b06d0a0394",
   "metadata": {},
   "source": [
    "### Stemning and It'type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c698573-0310-44d5-8fc2-2e8f68a494eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['eating','east','eaten','writing','writes','programming','programs','finally','finalize','history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41eb14-10f8-4c38-91e9-1cbee68278de",
   "metadata": {},
   "source": [
    "##### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "560f2417-2fa5-47f6-9e2e-968faa09f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9c7703-0738-487c-8032-9c1908ce433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ----> eat\n",
      "east ----> east\n",
      "eaten ----> eaten\n",
      "writing ----> write\n",
      "writes ----> write\n",
      "programming ----> program\n",
      "programs ----> program\n",
      "finally ----> final\n",
      "finalize ----> final\n",
      "history ----> histori\n",
      "congratulation----->congratul\n"
     ]
    }
   ],
   "source": [
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "    print(f'{word} ----> {stemming.stem(word)}')\n",
    "# It may change measning of words\n",
    "print(f'congratulation----->{stemming.stem(\"congratulation\")}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56247c63-485f-4923-924a-30acc1f2c5f0",
   "metadata": {},
   "source": [
    "##### RegexpStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5ae833-17d6-4a8c-869c-86006ca0812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082b25c2-9718-48d7-8a66-3144f3f79710",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_exp = RegexpStemmer('ing$|s$|e$|able$', min = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d581eb0d-ca16-47cd-9a15-2ecf6f872b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "words1= ['eating', 'unable', 'understanding','readable', 'plants']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea8be90-a804-4886-bc23-22286146b6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "un\n",
      "understand\n",
      "read\n",
      "plant\n"
     ]
    }
   ],
   "source": [
    "for word in words1:\n",
    "    print(reg_exp.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffe8e2d-044e-41f8-941b-f87b1702dc68",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4c42b3-09f4-4567-bd9d-27e03a007fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae809d21-4802-415e-b94e-488364a86241",
   "metadata": {},
   "outputs": [],
   "source": [
    "Snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdb476fe-138d-4ea3-8608-0ab7e6d08ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "east\n",
      "eaten\n",
      "write\n",
      "write\n",
      "program\n",
      "program\n",
      "final\n",
      "final\n",
      "histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(Snowball_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746e870f-84e6-4fd0-b783-4ab20fb6ebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"fairly\"), stemming.stem(\"sportingly\")  # Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d5bdbcf-a857-4212-9e5d-fbb47c21fa85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Snowball_stemmer.stem(\"fairly\"), Snowball_stemmer.stem(\"sportingly\")  # Snowball Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7f888-e52b-4f27-a5ce-3c57e9028a73",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "003757ec-7b66-44c7-93b7-788f34c375e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer  #it's a class which is a thin wrapper around the wordnet corpus as it uses morphy() to wordnet corpus reader to find lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2382489d-847a-458a-ae1f-2a450166602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81d53b6f-0098-4461-b640-d10f593e9cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "go\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"going\"))  # Default pos = n\n",
    "print(lemmatizer.lemmatize(\"going\", pos = 'v')) \n",
    "print(lemmatizer.lemmatize(\"going\", pos = 'a')) # Takes 2 parameter (word, part of speech)\n",
    "# Noun - n\n",
    "# Verb - v\n",
    "# Adjective - a\n",
    "# Adverb - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fa91c97-c5c5-46db-87f9-92e97b670666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating\n",
      "east\n",
      "eaten\n",
      "writing\n",
      "writes\n",
      "programming\n",
      "program\n",
      "finally\n",
      "finalize\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word,pos='n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d81fd-00ad-4248-b4c4-670049adf53a",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a62853b-b877-4918-941b-1452bd0b77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "690e3097-b3da-4b27-9c6d-a07eb9d84496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65bb1270-3e6b-4e3f-b14e-dc4f834bab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "232\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "print(len(stopwords.words('english')))\n",
    "print(len(stopwords.words('german')))\n",
    "print(len(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b0e66-a512-41a8-9780-52b0c1ba6304",
   "metadata": {},
   "source": [
    "#### Port Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b745b8e1-cd26-4604-8767-1bbc34d78645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent_tokenize(paragraph)\n",
    "for sent in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[sent])\n",
    "    words = [stemming.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[sent] = ' '.join(words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d59237-6a8b-4c4d-97d6-a9f3864c2696",
   "metadata": {},
   "source": [
    "#### SnowSnowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c83bfab2-ea48-4a7b-8b0b-5beb029c3ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i three vision india .',\n",
       " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .',\n",
       " 'my second vision india ’ develop .',\n",
       " 'for fifti year develop nation .',\n",
       " 'it time see develop nation .',\n",
       " 'we among top 5 nation world term gdp .',\n",
       " 'we 10 percent growth rate area .',\n",
       " 'our poverti level fall .',\n",
       " 'our achiev global recognis today .',\n",
       " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
       " 'isn ’ incorrect ?',\n",
       " 'i third vision .',\n",
       " 'india must stand world .',\n",
       " 'becaus i believ unless india stand world , one respect us .',\n",
       " 'onli strength respect strength .',\n",
       " 'we must strong militari power also econom power .',\n",
       " 'both must go hand-in-hand .',\n",
       " 'my good fortun work three great mind .',\n",
       " 'dr. vikram sarabhai dept .',\n",
       " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
       " 'i lucki work three close consid great opportun life .',\n",
       " 'i see four mileston career']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent_tokenize(paragraph)\n",
    "for sent in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[sent])\n",
    "    words = [Snowball_stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[sent] = ' '.join(words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d6ba43-3503-432c-a4c1-10dbe9e94899",
   "metadata": {},
   "source": [
    "#### Wordnet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68af2462-6ca4-4d14-a13e-c4690e4cda67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I three vision India .',\n",
       " 'In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
       " 'Yet done nation .',\n",
       " 'We conquered anyone .',\n",
       " 'We grabbed land , culture , history tried enforce way life .',\n",
       " 'Why ?',\n",
       " 'Because respect freedom others.That first vision freedom .',\n",
       " 'I believe India got first vision 1857 , started War Independence .',\n",
       " 'It freedom must protect nurture build .',\n",
       " 'If free , one respect u .',\n",
       " 'My second vision India ’ development .',\n",
       " 'For fifty year developing nation .',\n",
       " 'It time see developed nation .',\n",
       " 'We among top 5 nation world term GDP .',\n",
       " 'We 10 percent growth rate area .',\n",
       " 'Our poverty level falling .',\n",
       " 'Our achievement globally recognised today .',\n",
       " 'Yet lack self-confidence see developed nation , self-reliant self-assured .',\n",
       " 'Isn ’ incorrect ?',\n",
       " 'I third vision .',\n",
       " 'India must stand world .',\n",
       " 'Because I believe unless India stand world , one respect u .',\n",
       " 'Only strength respect strength .',\n",
       " 'We must strong military power also economic power .',\n",
       " 'Both must go hand-in-hand .',\n",
       " 'My good fortune worked three great mind .',\n",
       " 'Dr. Vikram Sarabhai Dept .',\n",
       " 'space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .',\n",
       " 'I lucky worked three closely consider great opportunity life .',\n",
       " 'I see four milestone career']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sent_tokenize(paragraph)\n",
    "for sent in range(len(sentence)):\n",
    "    words = word_tokenize(sentence[sent])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[sent] = ' '.join(words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564cf99-7b1c-44d7-9b12-ccd15a9a08fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
